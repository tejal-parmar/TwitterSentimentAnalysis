{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a1fa59",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7b81d6",
   "metadata": {},
   "source": [
    "## Capstone Project by Group-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ca377",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b75d3",
   "metadata": {},
   "source": [
    "#### <font color='blue'>Tweets Scrapping </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8fd84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9039fdf0",
   "metadata": {},
   "source": [
    "#### <font color='blue'>Data Structures</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f78a12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 1000\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c09607",
   "metadata": {},
   "source": [
    "#### <font color='blue'>Text Processing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e088dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import cleantext\n",
    "\n",
    "from better_profanity import profanity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7a732",
   "metadata": {},
   "source": [
    "#### <font color='blue'>ML Modeling</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70eb5594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_classifier = pipeline(\"sentiment-analysis\",\n",
    "                model = \"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d83c5",
   "metadata": {},
   "source": [
    "#### <font color='blue'>Plotting</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92880f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08182d18",
   "metadata": {},
   "source": [
    "#### <font color='blue'>User Interface</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0868c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "import cleantext\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec5f16f",
   "metadata": {},
   "source": [
    "### Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "864fc7c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def scrape_tweets(search_item):\n",
    "\n",
    "    tweets = []\n",
    "\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(search_item).get_items()):\n",
    "        if i > 200:\n",
    "            break\n",
    "        tweets.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "    df = tweets_df[['Text']]\n",
    "    df.columns = ['tweets']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e70098",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffceba5b",
   "metadata": {},
   "source": [
    "#### Parsing using Spacy <br>Flitering punctuation, white space, numbers, URL, @ mention using Spacy <br>Removing special character and search item <br>Single syllable token removal <br>Spell correction: dealing with repeated characters such as \"soooo gooood\", if the same character is repeated more than two times, it shortens the repeatition to two. Hence \"soooo gooood\" will be transformed to \"soo good\". It will help to reduce feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab4007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_cleaner(text, search_item):\n",
    "    parsed = nlp(text.lower())\n",
    "    final_tokens = []\n",
    "    for t in parsed:\n",
    "        if t.is_punct or t.is_space or t.like_num or t.like_url or str(t).startswith('@'):\n",
    "            pass\n",
    "        else:\n",
    "            sc_removed = re.sub(\"[^a-zA-Z]\", '', str(t.lemma_))\n",
    "            si_removed = sc_removed.replace(search_item, '')\n",
    "            if len(si_removed) > 1:\n",
    "                final_tokens.append(si_removed)\n",
    "\n",
    "    joined = ' '.join(final_tokens)\n",
    "    spell_corrected = re.sub(r'(.)\\1+', r'\\1\\1', joined)\n",
    "    return spell_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ffe477",
   "metadata": {},
   "source": [
    "#### Removing empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1af873f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def data_cleaning(data, search_item):\n",
    "    data['clean_text'] = [spacy_cleaner(t, search_item) for t in data.tweets]\n",
    "    data['clean_text'] = data['clean_text'].replace('', np.nan)\n",
    "    cleaned_data = data.dropna(subset = ['clean_text'])\n",
    "    cleaned_data.reset_index(drop = True,inplace = True)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3091ca",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc1b00",
   "metadata": {},
   "source": [
    "#### A word cloud represents word usage in a document by resizing individual words proportionally to its frequency, and then presenting them in a random arrangement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c216f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cloud(cleaned_data, search_item):\n",
    "    stp_words = stopwords.words('english')\n",
    "    stp_words.append('amp')\n",
    "    joined = ' '.join(word for word in cleaned_data['clean_text'])\n",
    "    consolidated = ' '.join(word for word in joined.split() if not profanity.contains_profanity(word))\n",
    "    wordCloud = WordCloud(width = 1600, height = 800,\n",
    "                          max_font_size = 200, max_words = 100,\n",
    "                          background_color = 'White', collocations = False, stopwords = stp_words).generate(consolidated)\n",
    "    plt.imshow(wordCloud, interpolation = 'bilinear')\n",
    "    plt.axis('off')\n",
    "    file_name = search_item + \"_WordCloud.png\"\n",
    "    wordCloud.to_file(file_name)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873aced1",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c914e9",
   "metadata": {},
   "source": [
    "#### After testing 4 popular sentiment analysis libraries in Python: NLTK, TextBlob, Flair, and HuggingFace transformers. <br> We manually validated the predicted labels for each tweet and HuggingFace transformers (Twitter_roberta) proven to be the most accurate model for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5200237c",
   "metadata": {},
   "source": [
    "#### The following code uses twitter_roberta model to classify the sentiment of each tweet in the dataset, the resulted dataframe will have an additional column containing the predicted label for that particular tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c20ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling(cleaned_data, search_item):\n",
    "    results = cleaned_data.copy()\n",
    "    label, score = list(), list()\n",
    "    for tweet in results['clean_text']:\n",
    "        sentiment = sentiment_classifier(tweet)\n",
    "        label.append(sentiment[0]['label'])\n",
    "        score.append(sentiment[0]['score'])\n",
    "    \n",
    "    results['Label'] = label\n",
    "    results['Score'] = score\n",
    "    \n",
    "    file_name = search_item + \"_clean_text_tweet_results.csv\"\n",
    "    results.to_csv(file_name)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58cbbeb",
   "metadata": {},
   "source": [
    "### Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "432181e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_segmentation(col, results):\n",
    "    seg_df = results.pivot_table(index = [col], aggfunc = {col:'count'})\n",
    "    mylabels = np.sort(results[col].unique())\n",
    "    if len(mylabels) > 2:\n",
    "        colors = ['#E44144', '#7CADD2', '#509B51']\n",
    "    else:\n",
    "        colors = ['#E44144', '#509B51']\n",
    "    plt.pie(np.array(seg_df).ravel(), labels = mylabels, autopct = \"%1.1f%%\", colors = colors)\n",
    "    plt.legend(title = \"Sentiments:\", bbox_to_anchor = (1.05, 1), loc = 'upper left')\n",
    "    file_name = col + \"_pieplot.png\"\n",
    "    plt.savefig(file_name, dpi = 300)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b094741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPie(labels, values):\n",
    "    fig = go.Figure(\n",
    "        go.Pie(\n",
    "        labels = labels,\n",
    "        values = [value*100 for value in values],\n",
    "        hoverinfo = \"label+percent\",\n",
    "        textinfo = \"value\"\n",
    "    ))\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "lastSearched = \"\"\n",
    "cacheData = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3525c2",
   "metadata": {},
   "source": [
    "### User Interface and Dashboard using Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fc9cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_convert(sec):\n",
    "    mins = int(sec//60)\n",
    "    sec = round(sec%60, 2)\n",
    "    hours = int(mins//60)\n",
    "    time_lapsed = ''\n",
    "    \n",
    "    if hours == 0:\n",
    "        if mins == 0:\n",
    "            time_lapsed = str(sec) + 's'\n",
    "        else:\n",
    "            time_lapsed = str(mins) + 'm ' + str(sec) + 's'\n",
    "    else:\n",
    "        time_lapsed = str(hours) + 'h ' + str(mins) + 'm ' + str(sec) + 's'\n",
    "    return time_lapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf91aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    st.title(\"Twitter Sentiment Analysis\")\n",
    "    st.text(\"Analyze sentiments of a popular hashtag or topic on Twitter\")\n",
    "    \n",
    "    search_item = st.text_input('Search Topic', placeholder = 'Input keyword HERE')\n",
    "\n",
    "    placeholder = st.empty()\n",
    "    btn = placeholder.button('Analyze', disabled = False, key = \"1\")\n",
    "\n",
    "    if btn:\n",
    "        if search_item:\n",
    "            start_time = time.time()\n",
    "        \n",
    "            placeholder.button('Analyze', disabled = True, key = \"2\")\n",
    "            btn = st.empty()\n",
    "            \n",
    "            print('Analyzing..')\n",
    "\n",
    "            data = scrape_tweets(search_item)\n",
    "            print('data scraped..')\n",
    "\n",
    "            cleaned_data = data_cleaning(data, search_item)\n",
    "            print('data cleaned..')\n",
    "\n",
    "            model_results = modeling(cleaned_data, search_item)\n",
    "            print('classification completed..')\n",
    "\n",
    "            st.subheader(\"Analysis of latest {} Tweets\".format(len(model_results)))\n",
    "\n",
    "            col1, col2 = st.columns(2)\n",
    "            with col1:\n",
    "                fig_cloud = create_cloud(model_results, search_item)\n",
    "                print('wordcloud created')\n",
    "                st.subheader(\"WordCloud of 100 words\")\n",
    "                st.pyplot(fig_cloud)\n",
    "                \n",
    "            with col2:\n",
    "                fig = plt_segmentation('Label', model_results)\n",
    "                print('segmentation plot created')\n",
    "                st.subheader(\"Sentiment by Percent\")\n",
    "                st.pyplot(fig)\n",
    "\n",
    "            pos_df = model_results[(model_results.Label == 'positive')].sort_values(by=['Score'], ascending = False)\n",
    "            pos_df['censored'] = [profanity.censor(t) for t in pos_df.tweets]\n",
    "\n",
    "            neg_df = model_results[(model_results.Label == 'negative')].sort_values(by=['Score'], ascending = False)\n",
    "            neg_df['censored'] = [profanity.censor(t) for t in neg_df.tweets]\n",
    "\n",
    "            col3, col4 = st.columns(2)\n",
    "            with col3:\n",
    "                st.subheader(\":green[{} Positive Tweets]\".format(len(pos_df)))\n",
    "                for i in pos_df['censored'].head(3):\n",
    "                    st.markdown(f'<h3 style=\"color:green;font-size:14px;\">{i}</h3>', unsafe_allow_html=True)\n",
    "\n",
    "            with col4:\n",
    "                st.subheader(\":red[{} Negative Tweets]\".format(len(neg_df)))\n",
    "                for i in neg_df['censored'].head(3):\n",
    "                    st.markdown(f'<h3 style=\"color:red;font-size:14px;\">{i}</h3>', unsafe_allow_html=True) \n",
    "\n",
    "            end_time = time.time()\n",
    "            time_lapsed = time_convert(end_time - start_time)\n",
    "            \n",
    "            st.success('Analysis finished in {}'.format(time_lapsed))\n",
    "            print('Analysis is finished in {}'.format(time_lapsed))\n",
    "            \n",
    "            btn1 = st.button('Clear')\n",
    "            \n",
    "            if btn1:\n",
    "                st.experimental_singleton.clear()\n",
    "                run()\n",
    "        else:\n",
    "            st.warning(\"Please enter a keyword to analyze\")\n",
    "\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
